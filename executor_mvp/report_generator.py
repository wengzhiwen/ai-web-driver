"""LLM-powered test report generator for human-readable test summaries."""
from __future__ import annotations

import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

from compiler_mvp.llm_client import LLMClient, LLMClientError
from .models import ActionPlan, RunResult


class TestReportGenerator:
    """Generates human-readable test reports using LLM analysis."""

    def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
        """Initialize the report generator.

        Args:
            llm_client: Optional LLM client for generating reports. If None, creates a default client.
        """
        self.llm_client = llm_client or LLMClient()
        self.logger = logging.getLogger(__name__)

    def generate_report(
        self,
        plan: ActionPlan,
        result: RunResult,
        output_path: Optional[Path] = None
    ) -> str:
        """Generate a human-readable test report.

        Args:
            plan: The original ActionPlan that was executed
            result: The execution result from the executor
            output_path: Optional path to save the report

        Returns:
            The generated report as a string
        """
        try:
            # Prepare analysis context
            analysis_context = self._prepare_analysis_context(plan, result)

            # Generate report using LLM
            report = self._generate_llm_report(analysis_context)

            # Save report if path provided
            if output_path:
                output_path.parent.mkdir(parents=True, exist_ok=True)
                output_path.write_text(report, encoding="utf-8")
                self.logger.info("Test report saved to %s", output_path)

            return report

        except LLMClientError as exc:
            self.logger.warning("LLM report generation failed, falling back to template: %s", exc)
            return self._generate_fallback_report(plan, result)
        except Exception as exc:
            self.logger.error("Unexpected error generating report: %s", exc)
            return self._generate_fallback_report(plan, result)

    def _prepare_analysis_context(self, plan: ActionPlan, result: RunResult) -> Dict[str, object]:
        """Prepare context data for LLM analysis."""

        # Categorize steps by status
        passed_steps = [step for step in result.steps if step.status == "passed"]
        failed_steps = [step for step in result.steps if step.status == "failed"]

        # Extract key actions
        navigation_steps = []
        interaction_steps = []
        assertion_steps = []
        verification_points = []

        for i, step in enumerate(plan.steps):
            if step.t == "goto":
                navigation_steps.append(f"æ­¥éª¤{i+1}: å¯¼èˆªåˆ° {step.url}")
            elif step.t in ["fill", "click"]:
                action_desc = f"æ­¥éª¤{i+1}: {self._describe_action(step)}"
                interaction_steps.append(action_desc)
            elif step.t == "assert":
                assertion_desc = f"æ­¥éª¤{i+1}: {self._describe_assertion(step)}"
                assertion_steps.append(assertion_desc)
                # Also collect verification points for detailed listing
                verification_point = self._describe_assertion(step)
                verification_points.append(verification_point)

        # Calculate execution metrics
        total_duration = (result.finished_at - result.started_at).total_seconds()
        passed_count = len(passed_steps)
        failed_count = len(failed_steps)
        success_rate = (passed_count / len(result.steps)) * 100 if result.steps else 0

        # Extract page flow information
        page_flow = self._extract_page_flow(result)

        # Identify test objectives
        test_objectives = self._infer_test_objectives(plan)

        return {
            "test_info": {
                "test_id": result.test_id,
                "run_id": result.run_id,
                "status": result.status,
                "success_rate": f"{success_rate:.1f}%",
                "total_duration": f"{total_duration:.2f}ç§’",
                "total_steps": len(result.steps),
                "passed_steps": passed_count,
                "failed_steps": failed_count,
                "base_url": plan.base_url or "æœªçŸ¥",
                "executed_at": result.started_at.strftime("%Y-%m-%d %H:%M:%S"),
            },
            "test_objectives": test_objectives,
            "page_flow": page_flow,
            "navigation_actions": navigation_steps,
            "interaction_actions": interaction_steps,
            "verification_points": assertion_steps,
            "detailed_verification_list": verification_points,
            "execution_summary": {
                "key_achievements": self._extract_key_achievements(plan, result),
                "failure_analysis": self._analyze_failures(failed_steps) if failed_steps else [],
                "performance_metrics": self._calculate_performance_metrics(result),
            },
            "detailed_steps": self._format_detailed_steps(plan, result),
        }

    def _describe_action(self, step) -> str:
        """Describe an action step in natural language."""
        if step.t == "fill":
            return f"åœ¨ {step.selector} ä¸­è¾“å…¥ '{step.value}'"
        elif step.t == "click":
            return f"ç‚¹å‡» {step.selector}"
        else:
            return f"æ‰§è¡Œ {step.t} æ“ä½œ"

    def _describe_assertion(self, step) -> str:
        """Describe an assertion step in natural language."""
        if step.kind == "visible":
            return f"éªŒè¯ {step.selector} å¯è§"
        elif step.kind == "text_contains":
            return f"éªŒè¯ {step.selector} åŒ…å«æ–‡æœ¬ '{step.value}'"
        elif step.kind == "text_equals":
            return f"éªŒè¯ {step.selector} æ–‡æœ¬ç­‰äº '{step.value}'"
        elif step.kind == "count_equals":
            return f"éªŒè¯ {step.selector} æ•°é‡ç­‰äº {step.value}"
        elif step.kind == "count_at_least":
            return f"éªŒè¯ {step.selector} æ•°é‡è‡³å°‘ {step.value}"
        else:
            return f"éªŒè¯ {step.selector} {step.kind}"

    def _extract_page_flow(self, result: RunResult) -> List[str]:
        """Extract page navigation flow from execution results."""
        page_flow = []
        current_url = None

        for step in result.steps:
            if step.current_url and step.current_url != current_url:
                current_url = step.current_url
                page_title = step.page_title or "æœªçŸ¥é¡µé¢"
                page_flow.append(f"{page_title} ({current_url})")

        return page_flow

    def _infer_test_objectives(self, plan: ActionPlan) -> List[str]:
        """Infer test objectives from the action plan."""
        objectives = []

        # Look for search functionality
        has_search = any(step.t == "fill" and "search" in step.selector.lower()
                        for step in plan.steps)
        if has_search:
            objectives.append("éªŒè¯æœç´¢åŠŸèƒ½")

        # Look for navigation
        has_navigation = any(step.t == "goto" for step in plan.steps)
        if has_navigation:
            objectives.append("éªŒè¯é¡µé¢å¯¼èˆª")

        # Look for form interactions
        has_form = any(step.t == "fill" for step in plan.steps)
        if has_form:
            objectives.append("éªŒè¯è¡¨å•äº¤äº’")

        # Extract specific verification points
        verification_points = []
        for step in plan.steps:
            if step.t == "assert":
                verification_point = self._describe_assertion(step)
                verification_points.append(verification_point)

        if verification_points:
            objectives.append(f"éªŒè¯{len(verification_points)}ä¸ªå…·ä½“æ£€æŸ¥ç‚¹")

        return objectives

    def _extract_key_achievements(self, plan: ActionPlan, result: RunResult) -> List[str]:
        """Extract key achievements from successful execution."""
        achievements = []

        if result.status == "passed":
            achievements.append("âœ… æ‰€æœ‰æµ‹è¯•æ­¥éª¤æ‰§è¡ŒæˆåŠŸ")

            # Check specific achievements
            if any(step.t == "fill" for step in plan.steps):
                achievements.append("âœ… è¡¨å•å¡«å†™åŠŸèƒ½æ­£å¸¸")

            if any(step.t == "click" for step in plan.steps):
                achievements.append("âœ… é¡µé¢äº¤äº’åŠŸèƒ½æ­£å¸¸")

            assertion_count = sum(1 for step in plan.steps if step.t == "assert")
            if assertion_count > 0:
                achievements.append(f"âœ… {assertion_count}ä¸ªéªŒè¯ç‚¹å…¨éƒ¨é€šè¿‡")

        return achievements

    def _analyze_failures(self, failed_steps: List) -> List[str]:
        """Analyze failed steps and provide insights."""
        failure_analysis = []

        for step in failed_steps:
            if step.error:
                if "æœªèƒ½æ‰¾åˆ°æŒ‡å®šçš„DOMå…ƒç´ " in step.error:
                    failure_analysis.append(f"é¡µé¢å…ƒç´ å®šä½å¤±è´¥: {step.selector}")
                elif "timeout" in step.error.lower():
                    failure_analysis.append(f"æ“ä½œè¶…æ—¶: {step.t} {step.selector}")
                else:
                    failure_analysis.append(f"æ‰§è¡Œé”™è¯¯: {step.error}")

        return failure_analysis

    def _calculate_performance_metrics(self, result: RunResult) -> Dict[str, str]:
        """Calculate performance metrics from execution results."""
        total_duration = (result.finished_at - result.started_at).total_seconds()

        if result.steps:
            avg_step_time = total_duration / len(result.steps)
        else:
            avg_step_time = 0

        return {
            "æ€»æ‰§è¡Œæ—¶é—´": f"{total_duration:.2f}ç§’",
            "å¹³å‡æ¯æ­¥æ—¶é—´": f"{avg_step_time:.2f}ç§’",
            "æ€»æ­¥éª¤æ•°": str(len(result.steps)),
        }

    def _format_detailed_steps(self, plan: ActionPlan, result: RunResult) -> List[Dict[str, object]]:
        """Format detailed step information for the report."""
        detailed_steps = []

        for i, (plan_step, result_step) in enumerate(zip(plan.steps, result.steps), start=1):
            step_info = {
                "step_number": i,
                "action_type": plan_step.t,
                "selector": plan_step.selector or "N/A",
                "status": result_step.status,
                "duration": f"{(result_step.finished_at - result_step.started_at).total_seconds():.3f}ç§’",
                "page_url": result_step.current_url or "N/A",
                "page_title": result_step.page_title or "N/A",
            }

            if result_step.status == "failed" and result_step.error:
                step_info["error"] = result_step.error

            if plan_step.t in ["fill", "assert"] and plan_step.value:
                step_info["value"] = plan_step.value

            detailed_steps.append(step_info)

        return detailed_steps

    def _generate_llm_report(self, analysis_context: Dict[str, object]) -> str:
        """Generate report using LLM analysis."""

        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è½¯ä»¶æµ‹è¯•å·¥ç¨‹å¸ˆï¼Œè´Ÿè´£ç”Ÿæˆæ¸…æ™°ã€ä¸“ä¸šçš„æµ‹è¯•æ‰§è¡ŒæŠ¥å‘Šã€‚

è¯·åŸºäºæä¾›çš„æµ‹è¯•æ‰§è¡Œæ•°æ®ï¼Œç”Ÿæˆä¸€ä»½äººç±»å¯è¯»çš„ç®€æ˜æµ‹è¯•æŠ¥å‘Šã€‚æŠ¥å‘Šåº”è¯¥ï¼š

1. **ç»“æ„æ¸…æ™°**ï¼šä½¿ç”¨é€‚å½“çš„æ ‡é¢˜å’Œåˆ†æ®µ
2. **è¯­è¨€ç®€æ´**ï¼šç”¨ä¸“ä¸šä½†æ˜“æ‡‚çš„è¯­è¨€æè¿°æµ‹è¯•ç»“æœ
3. **é‡ç‚¹çªå‡º**ï¼šçªå‡ºæµ‹è¯•çš„æˆåŠŸç‚¹å’Œå…³é”®å‘ç°
4. **å®ç”¨æ€§å¼º**ï¼šè®©è¯»è€…èƒ½å¤Ÿå¿«é€Ÿäº†è§£æµ‹è¯•æ˜¯å¦æˆåŠŸå®Œæˆ
5. **è¯¦ç»†éªŒè¯**ï¼šå¿…é¡»åˆ—å‡ºæ‰€æœ‰å…·ä½“çš„éªŒè¯ç‚¹ï¼Œä¸è¦åªç»™å‡ºè®¡æ•°

**é‡è¦è¦æ±‚**ï¼š
- å¿…é¡»åœ¨æŠ¥å‘Šä¸­ä¸“é—¨åˆ—å‡ºæ‰€æœ‰éªŒè¯æ£€æŸ¥ç‚¹ï¼Œè®©è¯»è€…æ¸…æ¥šçŸ¥é“å…·ä½“éªŒè¯äº†ä»€ä¹ˆ
- ä¸è¦åªè¯´"éªŒè¯äº†10ä¸ªæ£€æŸ¥ç‚¹"ï¼Œè€Œè¦åˆ—å‡ºè¿™10ä¸ªæ£€æŸ¥ç‚¹å…·ä½“æ˜¯ä»€ä¹ˆ
- ç”¨æ¸…æ™°çš„è¯­è¨€æè¿°æ¯ä¸ªéªŒè¯ç‚¹çš„å…·ä½“å†…å®¹å’ŒéªŒè¯ç»“æœ

æŠ¥å‘Šæ ¼å¼è¦æ±‚ï¼š
- ä½¿ç”¨Markdownæ ¼å¼
- åŒ…å«é€‚å½“çš„è¡¨æƒ…ç¬¦å·å¢å¼ºå¯è¯»æ€§
- è¯­æ°”ç§¯æä½†å®¢è§‚
- é‡ç‚¹ä¿¡æ¯åŠ ç²—æ˜¾ç¤º
- é€‚å½“æ—¶ä½¿ç”¨åˆ—è¡¨å±•ç¤ºä¿¡æ¯
- éªŒè¯ç‚¹éƒ¨åˆ†ä½¿ç”¨ç¼–å·åˆ—è¡¨è¯¦ç»†åˆ—å‡º

è¯·ç”Ÿæˆä¸€ä»½è®©æµ‹è¯•è´Ÿè´£äººèƒ½å¤Ÿå®‰å¿ƒç¡®è®¤æµ‹è¯•å®Œæˆè´¨é‡çš„è¯¦ç»†æŠ¥å‘Šã€‚"""

        user_prompt = f"""è¯·åŸºäºä»¥ä¸‹æµ‹è¯•æ‰§è¡Œæ•°æ®ç”Ÿæˆç®€æ˜æµ‹è¯•æŠ¥å‘Šï¼š

## æµ‹è¯•åŸºæœ¬ä¿¡æ¯
- æµ‹è¯•ID: {analysis_context['test_info']['test_id']}
- è¿è¡ŒID: {analysis_context['test_info']['run_id']}
- æ‰§è¡ŒçŠ¶æ€: {analysis_context['test_info']['status']}
- æˆåŠŸç‡: {analysis_context['test_info']['success_rate']}
- æ‰§è¡Œæ—¶é—´: {analysis_context['test_info']['total_duration']}
- æ‰§è¡Œå¼€å§‹æ—¶é—´: {analysis_context['test_info']['executed_at']}

## æµ‹è¯•ç›®æ ‡
{chr(10).join(f"- {obj}" for obj in analysis_context['test_objectives'])}

## é¡µé¢è®¿é—®æµç¨‹
{chr(10).join(f"{i+1}. {page}" for i, page in enumerate(analysis_context['page_flow']))}

## å…³é”®æ“ä½œ
- å¯¼èˆªæ“ä½œ: {len(analysis_context['navigation_actions'])}ä¸ª
- äº¤äº’æ“ä½œ: {len(analysis_context['interaction_actions'])}ä¸ª
- éªŒè¯æ£€æŸ¥: {len(analysis_context['verification_points'])}ä¸ª

## å…·ä½“éªŒè¯ç‚¹æ¸…å•
{chr(10).join(f"- {point}" for point in analysis_context['detailed_verification_list'])}

## æ‰§è¡Œæ‘˜è¦
- ä¸»è¦æˆå°±: {chr(10).join(f"- {achievement}" for achievement in analysis_context['execution_summary']['key_achievements'])}
- æ€§èƒ½æŒ‡æ ‡: {chr(10).join(f"- {k}: {v}" for k, v in analysis_context['execution_summary']['performance_metrics'].items())}

è¯·ç”Ÿæˆä¸€ä»½ä¸“ä¸šã€æ¸…æ™°çš„æµ‹è¯•æŠ¥å‘Šã€‚"""

        try:
            response = self.llm_client.chat_completion(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.3
            )
            return response.strip()
        except Exception as exc:
            self.logger.error("LLM request failed: %s", exc)
            raise LLMClientError(f"LLM request failed: {exc}")

    def _generate_fallback_report(self, plan: ActionPlan, result: RunResult) -> str:
        """Generate a fallback report without LLM assistance."""

        total_duration = (result.finished_at - result.started_at).total_seconds()
        passed_count = len([s for s in result.steps if s.status == "passed"])
        failed_count = len([s for s in result.steps if s.status == "failed"])

        report = f"""# ğŸ§ª æµ‹è¯•æ‰§è¡ŒæŠ¥å‘Š

## ğŸ“Š æµ‹è¯•æ¦‚è§ˆ
- **æµ‹è¯•ID**: {result.test_id}
- **è¿è¡ŒçŠ¶æ€**: {'âœ… é€šè¿‡' if result.status == 'passed' else 'âŒ å¤±è´¥'}
- **æ‰§è¡Œæ—¶é—´**: {total_duration:.2f}ç§’
- **æ­¥éª¤ç»Ÿè®¡**: {passed_count}é€šè¿‡ / {failed_count}å¤±è´¥

## ğŸ¯ æ‰§è¡Œæ‘˜è¦
{'æ‰€æœ‰æµ‹è¯•æ­¥éª¤æˆåŠŸæ‰§è¡Œå®Œæˆã€‚' if result.status == 'passed' else 'æµ‹è¯•æ‰§è¡Œè¿‡ç¨‹ä¸­é‡åˆ°å¤±è´¥æ­¥éª¤ã€‚'}

## ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡
- æ€»æ‰§è¡Œæ—¶é—´: {total_duration:.2f}ç§’
- å¹³å‡æ¯æ­¥æ—¶é—´: {total_duration/len(result.steps):.2f}ç§’

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""

        return report